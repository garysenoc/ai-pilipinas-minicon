{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Technique 1: Basic Conversation Buffer Memory - Step by Step Guide\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a step-by-step guide to implementing **Basic Conversation Buffer Memory** using LangChain's modern LCEL (LangChain Expression Language) pattern.\n",
        "\n",
        "### What is Buffer Memory?\n",
        "\n",
        "Buffer memory is the simplest form of conversational history:\n",
        "- Stores **all messages** in a buffer\n",
        "- Passes **complete conversation history** to the LLM on each call\n",
        "- No compression or summarization\n",
        "- **Preserves complete context**\n",
        "\n",
        "### Key Benefits\n",
        "- ✅ Simple and straightforward\n",
        "- ✅ Preserves complete conversation context\n",
        "- ✅ No information loss\n",
        "- ✅ Uses modern LangChain v1.0+ patterns\n",
        "\n",
        "### Trade-offs\n",
        "- ⚠️ Can become expensive with long conversations (more tokens)\n",
        "- ⚠️ May hit token limits with very long conversations\n",
        "- ⚠️ No automatic summarization or compression\n",
        "\n",
        "### Use Case\n",
        "Short to medium-length conversations where you need complete context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "First, let's import all the necessary libraries for this implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Utilities\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import sys\n",
        "from typing import Dict\n",
        "\n",
        "# Token counting utilities\n",
        "import pathlib\n",
        "sys.path.append(str(pathlib.Path().absolute().parent))\n",
        "from utils.token_counter import (\n",
        "    count_tokens, \n",
        "    count_messages_tokens,\n",
        "    print_token_stats,\n",
        "    print_token_summary\n",
        ")\n",
        "\n",
        "# Load environment variables (for API keys)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"✅ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding ChatMessageHistory\n",
        "\n",
        "The `ChatMessageHistory` class is LangChain's built-in implementation that:\n",
        "1. Stores all messages in a list\n",
        "2. Provides methods to add messages (`add_message`)\n",
        "3. Returns all messages when accessed (`messages` property)\n",
        "\n",
        "Let's examine how it works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple chat message history instance\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "print(f\"Type: {type(history).__name__}\")\n",
        "print(f\"Current messages: {len(history.messages)}\")\n",
        "print(f\"Messages: {history.messages}\")\n",
        "\n",
        "# Add a message\n",
        "history.add_message(HumanMessage(content=\"Hello!\"))\n",
        "print(f\"\\nAfter adding a message:\")\n",
        "print(f\"Messages: {len(history.messages)}\")\n",
        "print(f\"First message: {history.messages[0].content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Session History Store\n",
        "\n",
        "We need a way to store and retrieve chat histories for different sessions. This allows multiple conversations to run independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store for chat message histories (session_id -> history)\n",
        "store: Dict[str, BaseChatMessageHistory] = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    \"\"\"\n",
        "    Get or create a chat message history for a session.\n",
        "    \n",
        "    This function is called by RunnableWithMessageHistory to retrieve\n",
        "    the history for a specific session. Each session gets its own\n",
        "    independent conversation history.\n",
        "    \"\"\"\n",
        "    if session_id not in store:\n",
        "        # Create a new ChatMessageHistory instance for this session\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    \n",
        "    return store[session_id]\n",
        "\n",
        "# Test the function\n",
        "test_history = get_session_history(\"test_session\")\n",
        "print(f\"✅ Created history for session: test_session\")\n",
        "print(f\"   Type: {type(test_history).__name__}\")\n",
        "print(f\"   Messages: {len(test_history.messages)}\")\n",
        "\n",
        "# Test with another session\n",
        "test_history2 = get_session_history(\"another_session\")\n",
        "print(f\"\\n✅ Created separate history for session: another_session\")\n",
        "print(f\"   Messages: {len(test_history2.messages)}\")\n",
        "print(f\"   Different instances: {test_history is not test_history2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create the LLM and Prompt Template\n",
        "\n",
        "Now we'll create the main LLM for conversations and a prompt template that includes a placeholder for message history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the main LLM for conversations\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.7,  # Higher temperature for more natural conversations\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Create a prompt template with message history placeholder\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Have a natural conversation with the user.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # This will be filled with all previous messages\n",
        "    (\"human\", \"{input}\")  # User's current input\n",
        "])\n",
        "\n",
        "print(\"✅ LLM and prompt template created!\")\n",
        "print(f\"   LLM Model: {llm.model_name}\")\n",
        "print(f\"   Prompt variables: {prompt.input_variables}\")\n",
        "print(f\"   History placeholder: 'history'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build the Chain with LCEL\n",
        "\n",
        "LCEL (LangChain Expression Language) allows us to chain components together using the `|` operator. We'll create a chain that:\n",
        "1. Takes the prompt template\n",
        "2. Pipes it to the LLM\n",
        "3. Wraps it with message history management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5.1: Create the base chain using LCEL\n",
        "# The | operator chains the prompt template to the LLM\n",
        "chain = prompt | llm\n",
        "\n",
        "print(\"✅ Base chain created using LCEL\")\n",
        "print(\"   Chain: prompt | llm\")\n",
        "print(\"   This chain takes input, formats it with the prompt, and sends it to the LLM\")\n",
        "\n",
        "# Step 5.2: Wrap with message history\n",
        "# RunnableWithMessageHistory automatically:\n",
        "# - Retrieves history using get_session_history\n",
        "# - Adds new messages to history after each call\n",
        "# - Passes ALL messages to the prompt template (buffer memory)\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,  # Function to get/create history for a session\n",
        "    input_messages_key=\"input\",  # Key for user input\n",
        "    history_messages_key=\"history\",  # Key for message history in prompt\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Chain wrapped with message history\")\n",
        "print(\"   Now the chain will automatically:\")\n",
        "print(\"   1. Retrieve conversation history for the session\")\n",
        "print(\"   2. Pass ALL messages to the LLM (buffer memory)\")\n",
        "print(\"   3. Add new messages to history after each call\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test the Implementation - First Message\n",
        "\n",
        "Let's test our implementation with a simple conversation to see how buffer memory works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new session for testing\n",
        "session_id = \"demo_session\"\n",
        "config = {\"configurable\": {\"session_id\": session_id}}\n",
        "\n",
        "# First message\n",
        "print(\"=\" * 60)\n",
        "print(\"Message 1: Introduction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hi, my name is Alice\"},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(f\"User: Hi, my name is Alice\")\n",
        "print(f\"Agent: {response1.content}\\n\")\n",
        "\n",
        "# Check the history\n",
        "history = get_session_history(session_id)\n",
        "print(f\"Messages stored in history: {len(history.messages)}\")\n",
        "print(f\"Message breakdown:\")\n",
        "for i, msg in enumerate(history.messages, 1):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        print(f\"  {i}. Human: {msg.content}\")\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        print(f\"  {i}. AI: {msg.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Test Memory - Second Message\n",
        "\n",
        "Now let's send a second message and see how the agent remembers the previous conversation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Message 2: Testing Memory\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(f\"User: What's my name?\")\n",
        "print(f\"Agent: {response2.content}\\n\")\n",
        "\n",
        "# Check the history - should now have 4 messages (2 human + 2 AI)\n",
        "history = get_session_history(session_id)\n",
        "print(f\"Total messages stored: {len(history.messages)}\")\n",
        "print(f\"\\nFull conversation history:\")\n",
        "for i, msg in enumerate(history.messages, 1):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        print(f\"  {i}. Human: {msg.content}\")\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        print(f\"  {i}. AI: {msg.content[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Understanding Buffer Memory Behavior\n",
        "\n",
        "With buffer memory, **all messages** are stored and passed to the LLM. Let's add more messages and observe how the history grows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add more messages to see buffer memory in action\n",
        "conversations = [\n",
        "    \"I'm a software engineer. What do I do?\",\n",
        "    \"What's my name again?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Adding More Messages\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, user_input in enumerate(conversations, 1):\n",
        "    print(f\"\\nMessage {i+2}: {user_input}\")\n",
        "    response = chain_with_history.invoke(\n",
        "        {\"input\": user_input},\n",
        "        config=config\n",
        "    )\n",
        "    print(f\"Agent: {response.content[:100]}...\\n\")\n",
        "    \n",
        "    history = get_session_history(session_id)\n",
        "    print(f\"  Total messages in buffer: {len(history.messages)}\")\n",
        "    print(f\"  (All messages are stored and passed to the LLM)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: View Complete Conversation History\n",
        "\n",
        "Let's see the complete conversation history stored in the buffer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = get_session_history(session_id)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Complete Conversation History (Buffer Memory)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total messages: {len(history.messages)}\\n\")\n",
        "\n",
        "for i, msg in enumerate(history.messages, 1):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        print(f\"{i}. Human: {msg.content}\")\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        print(f\"{i}. AI: {msg.content[:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Key Point: ALL messages are stored and passed to the LLM\")\n",
        "print(\"This is the 'buffer' - it keeps everything!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Understanding Token Usage\n",
        "\n",
        "With buffer memory, token usage grows with each message. Let's see how tokens accumulate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a fresh session for token counting\n",
        "session_id_tokens = \"token_demo\"\n",
        "config_tokens = {\"configurable\": {\"session_id\": session_id_tokens}}\n",
        "\n",
        "conversations = [\n",
        "    \"Hi, my name is Alice\",\n",
        "    \"What's my name?\",\n",
        "    \"I'm a software engineer. What do I do?\",\n",
        "    \"What's my name again?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Token Usage Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, user_input in enumerate(conversations, 1):\n",
        "    print(f\"\\n--- Turn {i} ---\")\n",
        "    print(f\"User: {user_input}\")\n",
        "    \n",
        "    # Count input tokens (user message + all history)\n",
        "    input_tokens = count_tokens(user_input)\n",
        "    history = get_session_history(session_id_tokens)\n",
        "    if history.messages:\n",
        "        input_tokens += count_messages_tokens(history.messages)\n",
        "    \n",
        "    response = chain_with_history.invoke(\n",
        "        {\"input\": user_input},\n",
        "        config=config_tokens\n",
        "    )\n",
        "    \n",
        "    output_tokens = count_tokens(response.content)\n",
        "    memory_tokens = count_messages_tokens(history.messages) if history.messages else 0\n",
        "    \n",
        "    print(f\"Agent: {response.content[:80]}...\")\n",
        "    print(f\"Input tokens: {input_tokens} (user message + {len(history.messages)} previous messages)\")\n",
        "    print(f\"Output tokens: {output_tokens}\")\n",
        "    print(f\"Memory tokens: {memory_tokens} (all stored messages)\")\n",
        "    \n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Notice: Token usage increases with each turn!\")\n",
        "print(\"This is because ALL messages are included in each request.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Complete Implementation Function\n",
        "\n",
        "Here's the complete function that combines all the steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_buffer_memory_agent():\n",
        "    \"\"\"Create an agent with basic buffer memory using modern LCEL pattern.\"\"\"\n",
        "    \n",
        "    # Initialize the LLM\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7,\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "    )\n",
        "    \n",
        "    # Create a prompt template with message history placeholder\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful AI assistant. Have a natural conversation with the user.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    \n",
        "    # Create the chain using LCEL\n",
        "    chain = prompt | llm\n",
        "    \n",
        "    # Wrap with message history (this provides the buffer memory)\n",
        "    chain_with_history = RunnableWithMessageHistory(\n",
        "        chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"history\",\n",
        "    )\n",
        "    \n",
        "    return chain_with_history\n",
        "\n",
        "print(\"✅ Complete implementation function created!\")\n",
        "print(\"\\nThis function:\")\n",
        "print(\"  1. Creates an LLM\")\n",
        "print(\"  2. Creates a prompt template with history placeholder\")\n",
        "print(\"  3. Chains them together with LCEL\")\n",
        "print(\"  4. Wraps with RunnableWithMessageHistory for automatic history management\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Full Demonstration with Token Counting\n",
        "\n",
        "Let's run a complete demonstration that shows token usage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_buffer_memory():\n",
        "    \"\"\"Demonstrate basic buffer memory with token counting.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Technique 1: Basic Conversation Buffer Memory (LCEL Pattern)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Using modern LangChain v1.0+ patterns\")\n",
        "    print()\n",
        "    \n",
        "    chain = create_buffer_memory_agent()\n",
        "    session_id = \"demo_session_full\"\n",
        "    config = {\"configurable\": {\"session_id\": session_id}}\n",
        "    \n",
        "    # Simulate a conversation\n",
        "    conversations = [\n",
        "        \"Hi, my name is Alice\",\n",
        "        \"What's my name?\",\n",
        "        \"I'm a software engineer. What do I do?\",\n",
        "        \"What's my name again?\"\n",
        "    ]\n",
        "    \n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    \n",
        "    for i, user_input in enumerate(conversations, 1):\n",
        "        print(f\"User: {user_input}\")\n",
        "        \n",
        "        # Count input tokens (user message + history)\n",
        "        input_tokens = count_tokens(user_input)\n",
        "        history = get_session_history(session_id)\n",
        "        if history.messages:\n",
        "            input_tokens += count_messages_tokens(history.messages)\n",
        "        total_input_tokens += input_tokens\n",
        "        \n",
        "        response = chain.invoke(\n",
        "            {\"input\": user_input},\n",
        "            config=config\n",
        "        )\n",
        "        print(f\"Agent: {response.content}\")\n",
        "        \n",
        "        # Count output tokens\n",
        "        output_tokens = count_tokens(response.content)\n",
        "        total_output_tokens += output_tokens\n",
        "        \n",
        "        # Count current memory tokens\n",
        "        history = get_session_history(session_id)\n",
        "        memory_tokens = count_messages_tokens(history.messages) if history.messages else 0\n",
        "        \n",
        "        print_token_stats(input_tokens, output_tokens, memory_tokens)\n",
        "        print()\n",
        "    \n",
        "    # Show the stored memory\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Stored Memory (All Messages):\")\n",
        "    print(\"-\" * 60)\n",
        "    history = get_session_history(session_id)\n",
        "    for message in history.messages:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"AI: {message.content}\")\n",
        "    print()\n",
        "    \n",
        "    # Show total token usage\n",
        "    final_memory = count_messages_tokens(history.messages) if history.messages else 0\n",
        "    print_token_summary(\n",
        "        total_input_tokens, \n",
        "        total_output_tokens, \n",
        "        final_memory\n",
        "    )\n",
        "\n",
        "# Uncomment to run the full demonstration\n",
        "# demonstrate_buffer_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
