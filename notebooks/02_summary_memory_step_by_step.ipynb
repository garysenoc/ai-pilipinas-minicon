{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Technique 2: Conversation Summary Memory - Step by Step Guide\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a step-by-step guide to implementing **Conversation Summary Memory** using LangChain's modern LCEL (LangChain Expression Language) pattern.\n",
        "\n",
        "### What is Summary Memory?\n",
        "\n",
        "Instead of storing all conversation messages (which can become expensive with long conversations), summary memory:\n",
        "- Maintains a **running summary** of older messages\n",
        "- Keeps only **recent messages** in full detail\n",
        "- Automatically **summarizes** when a threshold is reached\n",
        "- **Reduces token usage** for long conversations\n",
        "\n",
        "### Key Benefits\n",
        "- ✅ Efficient for long conversations\n",
        "- ✅ Automatically compresses information\n",
        "- ✅ Can handle very long conversation histories\n",
        "- ✅ Uses modern LangChain v1.0+ patterns\n",
        "\n",
        "### Trade-offs\n",
        "- ⚠️ Some detail may be lost in summarization\n",
        "- ⚠️ Requires additional LLM calls for summarization\n",
        "- ⚠️ Summary quality depends on the summarization prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "First, let's import all the necessary libraries for this implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Custom summary history implementation\n",
        "from utils.custom_chat_histories import SummaryChatMessageHistory\n",
        "\n",
        "# Utilities\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import sys\n",
        "from typing import Dict\n",
        "\n",
        "# Token counting utilities\n",
        "import pathlib\n",
        "sys.path.append(str(pathlib.Path().absolute().parent))\n",
        "from utils.token_counter import (\n",
        "    count_tokens, \n",
        "    count_messages_tokens,\n",
        "    print_token_stats,\n",
        "    print_token_summary\n",
        ")\n",
        "\n",
        "# Load environment variables (for API keys)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"✅ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding SummaryChatMessageHistory\n",
        "\n",
        "The `SummaryChatMessageHistory` class is a custom implementation that:\n",
        "1. Stores all messages internally (`_messages`)\n",
        "2. Automatically summarizes when a threshold is reached (default: 5 messages)\n",
        "3. Returns a summary + recent messages when accessed\n",
        "\n",
        "Let's examine how it works:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary LLM (used for summarization)\n",
        "summary_llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,  # Low temperature for consistent summarization\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Create a summary history instance\n",
        "summary_history = SummaryChatMessageHistory(summary_llm=summary_llm)\n",
        "\n",
        "print(f\"Summary threshold: {summary_history.summary_threshold} messages\")\n",
        "print(f\"Current messages: {len(summary_history._messages)}\")\n",
        "print(f\"Current summary: {summary_history.summary or '(empty)'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Session History Store\n",
        "\n",
        "We need a way to store and retrieve chat histories for different sessions. This allows multiple conversations to run independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store for chat message histories (session_id -> history)\n",
        "store: Dict[str, BaseChatMessageHistory] = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    \"\"\"\n",
        "    Get or create a summary chat message history for a session.\n",
        "    \n",
        "    This function is called by RunnableWithMessageHistory to retrieve\n",
        "    the history for a specific session.\n",
        "    \"\"\"\n",
        "    if session_id not in store:\n",
        "        # Create a new summary LLM for this session\n",
        "        summary_llm = ChatOpenAI(\n",
        "            model=\"gpt-4o\",\n",
        "            temperature=0,  # Low temperature for consistent summarization\n",
        "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "        )\n",
        "        # Create a new SummaryChatMessageHistory instance\n",
        "        store[session_id] = SummaryChatMessageHistory(summary_llm=summary_llm)\n",
        "    \n",
        "    return store[session_id]\n",
        "\n",
        "# Test the function\n",
        "test_history = get_session_history(\"test_session\")\n",
        "print(f\"✅ Created history for session: test_session\")\n",
        "print(f\"   Type: {type(test_history).__name__}\")\n",
        "print(f\"   Messages: {len(test_history.messages)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create the LLM and Prompt Template\n",
        "\n",
        "Now we'll create the main LLM for conversations and a prompt template that includes a placeholder for message history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the main LLM for conversations\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.7,  # Higher temperature for more natural conversations\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Create a prompt template with message history placeholder\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Have a natural conversation with the user.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # This will be filled with summary + recent messages\n",
        "    (\"human\", \"{input}\")  # User's current input\n",
        "])\n",
        "\n",
        "print(\"✅ LLM and prompt template created!\")\n",
        "print(f\"   LLM Model: {llm.model_name}\")\n",
        "print(f\"   Prompt variables: {prompt.input_variables}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build the Chain with LCEL\n",
        "\n",
        "LCEL (LangChain Expression Language) allows us to chain components together using the `|` operator. We'll create a chain that:\n",
        "1. Takes the prompt template\n",
        "2. Pipes it to the LLM\n",
        "3. Wraps it with message history management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5.1: Create the base chain using LCEL\n",
        "# The | operator chains the prompt template to the LLM\n",
        "chain = prompt | llm\n",
        "\n",
        "print(\"✅ Base chain created using LCEL\")\n",
        "print(\"   Chain: prompt | llm\")\n",
        "\n",
        "# Step 5.2: Wrap with message history\n",
        "# RunnableWithMessageHistory automatically:\n",
        "# - Retrieves history using get_session_history\n",
        "# - Adds new messages to history after each call\n",
        "# - Passes history to the prompt template\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,  # Function to get/create history for a session\n",
        "    input_messages_key=\"input\",  # Key for user input\n",
        "    history_messages_key=\"history\",  # Key for message history in prompt\n",
        ")\n",
        "\n",
        "print(\"✅ Chain wrapped with message history\")\n",
        "print(\"   Now the chain will automatically manage conversation history!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test the Implementation\n",
        "\n",
        "Let's test our implementation with a simple conversation to see how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new session for testing\n",
        "session_id = \"demo_session\"\n",
        "config = {\"configurable\": {\"session_id\": session_id}}\n",
        "\n",
        "# First message\n",
        "print(\"=\" * 60)\n",
        "print(\"Message 1\")\n",
        "print(\"=\" * 60)\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hi, I'm Bob and I work as a data scientist\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"User: Hi, I'm Bob and I work as a data scientist\")\n",
        "print(f\"Agent: {response1.content}\\n\")\n",
        "\n",
        "# Check the history\n",
        "history = get_session_history(session_id)\n",
        "print(f\"Messages in history: {len(history.messages)}\")\n",
        "print(f\"Summary: {history.summary or '(not created yet)'}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue the conversation\n",
        "print(\"=\" * 60)\n",
        "print(\"Message 2\")\n",
        "print(\"=\" * 60)\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"I specialize in machine learning and deep learning\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"User: I specialize in machine learning and deep learning\")\n",
        "print(f\"Agent: {response2.content}\\n\")\n",
        "\n",
        "history = get_session_history(session_id)\n",
        "print(f\"Messages in history: {len(history.messages)}\")\n",
        "print(f\"Summary: {history.summary or '(not created yet)'}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Understanding How Summarization Works\n",
        "\n",
        "The `SummaryChatMessageHistory` automatically summarizes when the threshold is reached. Let's add more messages to trigger summarization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add more messages to reach the summarization threshold (5 messages)\n",
        "conversations = [\n",
        "    \"I've been working on NLP projects for 5 years\",\n",
        "    \"My favorite programming language is Python\",\n",
        "    \"I enjoy working with neural networks\"\n",
        "]\n",
        "\n",
        "for i, user_input in enumerate(conversations, 1):\n",
        "    print(f\"Message {i+2}: {user_input}\")\n",
        "    response = chain_with_history.invoke(\n",
        "        {\"input\": user_input},\n",
        "        config=config\n",
        "    )\n",
        "    print(f\"Agent: {response.content[:100]}...\\n\")\n",
        "    \n",
        "    history = get_session_history(session_id)\n",
        "    print(f\"  Messages in history: {len(history.messages)}\")\n",
        "    print(f\"  Summary: {history.summary[:100] if history.summary else '(not created yet)'}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verify Summarization\n",
        "\n",
        "After reaching the threshold, older messages should be summarized. Let's check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = get_session_history(session_id)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"History Status After Multiple Messages\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total messages stored internally: {len(history._messages)}\")\n",
        "print(f\"Messages returned (summary + recent): {len(history.messages)}\")\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"{history.summary}\\n\" if history.summary else \"(No summary yet)\\n\")\n",
        "\n",
        "print(\"Recent messages returned:\")\n",
        "for i, msg in enumerate(history.messages, 1):\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        print(f\"  {i}. Human: {msg.content[:80]}...\")\n",
        "    else:\n",
        "        print(f\"  {i}. AI: {msg.content[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Memory Recall\n",
        "\n",
        "Now let's test if the agent remembers information from earlier in the conversation (which should be in the summary):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask about information from earlier in the conversation\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing Memory Recall\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "questions = [\n",
        "    \"What's my profession?\",\n",
        "    \"What programming language do I prefer?\",\n",
        "    \"How long have I been working on NLP?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\nUser: {question}\")\n",
        "    response = chain_with_history.invoke(\n",
        "        {\"input\": question},\n",
        "        config=config\n",
        "    )\n",
        "    print(f\"Agent: {response.content}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Complete Implementation Function\n",
        "\n",
        "Here's the complete function that combines all the steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_summary_memory_agent():\n",
        "    \"\"\"Create an agent with summary memory using LCEL pattern.\"\"\"\n",
        "    \n",
        "    # Initialize the LLM\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        temperature=0.7,\n",
        "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "    )\n",
        "    \n",
        "    # Create a prompt template with message history placeholder\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful AI assistant. Have a natural conversation with the user.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    \n",
        "    # Create the chain using LCEL\n",
        "    chain = prompt | llm\n",
        "    \n",
        "    # Wrap with message history (summary history)\n",
        "    chain_with_history = RunnableWithMessageHistory(\n",
        "        chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"history\",\n",
        "    )\n",
        "    \n",
        "    return chain_with_history\n",
        "\n",
        "print(\"✅ Complete implementation function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Full Demonstration with Token Counting\n",
        "\n",
        "Let's run a complete demonstration that shows token usage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_summary_memory():\n",
        "    \"\"\"Demonstrate summary memory with token counting.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Technique 2: Conversation Summary Memory (LCEL Pattern)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Using modern LangChain v1.0+ patterns with RunnableWithMessageHistory\")\n",
        "    print()\n",
        "    \n",
        "    chain = create_summary_memory_agent()\n",
        "    session_id = \"demo_session_full\"\n",
        "    config = {\"configurable\": {\"session_id\": session_id}}\n",
        "    \n",
        "    # Simulate a longer conversation\n",
        "    conversations = [\n",
        "        \"Hi, I'm Bob and I work as a data scientist\",\n",
        "        \"I specialize in machine learning and deep learning\",\n",
        "        \"I've been working on NLP projects for 5 years\",\n",
        "        \"My favorite programming language is Python\",\n",
        "        \"What's my profession?\",\n",
        "        \"What programming language do I prefer?\",\n",
        "        \"How long have I been working on NLP?\"\n",
        "    ]\n",
        "    \n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    \n",
        "    for i, user_input in enumerate(conversations, 1):\n",
        "        print(f\"User: {user_input}\")\n",
        "        \n",
        "        # Count input tokens (user message + history/summary)\n",
        "        input_tokens = count_tokens(user_input)\n",
        "        history = get_session_history(session_id)\n",
        "        if history.messages:\n",
        "            input_tokens += count_messages_tokens(history.messages)\n",
        "        # Add summary tokens if exists\n",
        "        if hasattr(history, 'summary') and history.summary:\n",
        "            input_tokens += count_tokens(history.summary)\n",
        "        total_input_tokens += input_tokens\n",
        "        \n",
        "        response = chain.invoke(\n",
        "            {\"input\": user_input},\n",
        "            config=config\n",
        "        )\n",
        "        print(f\"Agent: {response.content}\")\n",
        "        \n",
        "        # Count output tokens\n",
        "        output_tokens = count_tokens(response.content)\n",
        "        total_output_tokens += output_tokens\n",
        "        \n",
        "        # Count current memory tokens (summary + messages)\n",
        "        history = get_session_history(session_id)\n",
        "        memory_tokens = count_messages_tokens(history.messages) if history.messages else 0\n",
        "        if hasattr(history, 'summary') and history.summary:\n",
        "            memory_tokens += count_tokens(history.summary)\n",
        "        \n",
        "        print_token_stats(input_tokens, output_tokens, memory_tokens)\n",
        "        print()\n",
        "    \n",
        "    # Show the stored summary\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Stored Summary:\")\n",
        "    print(\"-\" * 60)\n",
        "    history = get_session_history(session_id)\n",
        "    if hasattr(history, 'summary') and history.summary:\n",
        "        print(history.summary)\n",
        "    else:\n",
        "        print(\"(Summary will be created after more messages)\")\n",
        "    print(f\"\\nRecent Messages: {len(history.messages)}\")\n",
        "    print()\n",
        "    \n",
        "    # Show total token usage\n",
        "    final_memory = count_messages_tokens(history.messages) if history.messages else 0\n",
        "    if hasattr(history, 'summary') and history.summary:\n",
        "        final_memory += count_tokens(history.summary)\n",
        "    \n",
        "    print_token_summary(\n",
        "        total_input_tokens, \n",
        "        total_output_tokens, \n",
        "        final_memory\n",
        "    )\n",
        "\n",
        "# Uncomment to run the full demonstration\n",
        "# demonstrate_summary_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Concepts Learned:\n",
        "\n",
        "1. **SummaryChatMessageHistory**: Custom class that automatically summarizes older messages\n",
        "2. **RunnableWithMessageHistory**: Wraps a chain to automatically manage message history\n",
        "3. **LCEL Pattern**: Modern way to chain components using the `|` operator\n",
        "4. **Session Management**: Using a store dictionary to manage multiple conversation sessions\n",
        "5. **Token Efficiency**: Summary memory reduces token usage for long conversations\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "1. Messages are stored internally in `_messages`\n",
        "2. When threshold (5 messages) is reached, older messages are summarized\n",
        "3. The `messages` property returns: `[summary_message] + recent_messages`\n",
        "4. This allows the LLM to have context without using all tokens\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Try adjusting `summary_threshold` in `SummaryChatMessageHistory`\n",
        "- Experiment with different summarization prompts\n",
        "- Compare token usage with basic buffer memory"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
